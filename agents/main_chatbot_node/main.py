import os
import asyncio
from typing import Annotated, TypedDict, Sequence
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langgraph.graph.message import add_messages
from langgraph.graph import StateGraph, START, END

# ë…¸ë“œ í•¨ìˆ˜ë“¤ import
from coursera_node import search_coursera_courses
from internal_lecture_node import lecture_search
from certifications_node import search_certifications
from config import MODEL_NAME, TEMPERATURE

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# LLM ì´ˆê¸°í™”
api_key = os.getenv("OPENAI_API_KEY")
if not api_key:
    raise ValueError("OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

os.environ["OPENAI_API_KEY"] = api_key

llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)

# State ì •ì˜
class AgentState(TypedDict):
    messages: Annotated[Sequence[BaseMessage], add_messages]
    coursera_result: str
    internal_result: str
    certifications_result: str

# Supervisor prompt
supervisor_prompt = """You are a learning path recommendation system.

User Question: {user_question}
Key Keywords: [Extract technical terms in English from the question (e.g., "í´ë¼ìš°ë“œ"â†’"cloud")]
IMPORTANT: First extract the key English keyword from the user question:

Search Results:
- Coursera External Courses: {coursera_result}
- Internal Company Courses: {internal_result}  
- Certification Information: {certifications_result}

Please organize and respond in the following clean format:

ğŸ¯ [Keyword] Learning Recommendation

ğŸ“š Recommended Courses
1. **[Course Name]** (Internal/Coursera)
   - Level: Beginner/Intermediate/Advanced
   - Description: [Brief description]

ğŸ† Related Certifications (if available)
- **[Certification Name]**: [Issuing Organization]

ğŸ’¡ Learning Path
[Simple guide in basicâ†’advanced order]"""

supervisor_template = ChatPromptTemplate.from_template(supervisor_prompt)
supervisor_chain = supervisor_template | llm

# ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ë…¸ë“œ
async def parallel_search_node(state):
    """3ê°œ ê²€ìƒ‰ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰"""
    async def run_coursera():
        result = search_coursera_courses(state)
        return result["messages"][0].content
    
    async def run_internal():
        result = lecture_search(state)
        return result["messages"][0].content
    
    async def run_certifications():
        result = search_certifications(state)
        return result["messages"][0].content
    
    # ë³‘ë ¬ ì‹¤í–‰
    coursera_result, internal_result, cert_result = await asyncio.gather(
        run_coursera(),
        run_internal(), 
        run_certifications()
    )
    
    return {
        "coursera_result": coursera_result,
        "internal_result": internal_result,
        "certifications_result": cert_result
    }

def supervisor_node(state):
    """ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì¢… ë‹µë³€ ìƒì„±"""
    user_question = state["messages"][0].content
    
    final_response = supervisor_chain.invoke({
        "user_question": user_question,
        "coursera_result": state.get("coursera_result", "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"),
        "internal_result": state.get("internal_result", "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"), 
        "certifications_result": state.get("certifications_result", "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ")
    })
    
    return {"messages": [AIMessage(content=final_response.content)]}

def create_graph():
    """ê·¸ë˜í”„ ìƒì„± - ì§„ì§œ ë³‘ë ¬ ì²˜ë¦¬"""
    workflow = StateGraph(AgentState)
    
    # ë…¸ë“œ ì¶”ê°€
    workflow.add_node("parallel_search", parallel_search_node)
    workflow.add_node("supervisor", supervisor_node)
    
    # ìˆœì°¨ ì‹¤í–‰: START â†’ parallel_search â†’ supervisor â†’ END
    workflow.add_edge(START, "parallel_search")
    workflow.add_edge("parallel_search", "supervisor")
    workflow.add_edge("supervisor", END)
    
    return workflow.compile()

if __name__ == "__main__":
    import time
    
    # 1. ê° ë…¸ë“œ ê²°ê³¼ê°’ ë³´ì—¬ì£¼ëŠ” í…ŒìŠ¤íŠ¸
    async def test_each_node():
        print("=== ê° ë…¸ë“œë³„ ê²°ê³¼ í…ŒìŠ¤íŠ¸ ===")
        query = "í´ë¼ìš°ë“œ ì–´ë–»ê²Œ ê³µë¶€í•˜ì§€?"
        test_state = {
            "messages": [HumanMessage(content=query)],
            "coursera_result": "",
            "internal_result": "",
            "certifications_result": ""
        }
        
        start_time = time.time()
        result = await parallel_search_node(test_state)
        end_time = time.time()
        
        print(f"ì‹¤í–‰ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ\n")
        print("ğŸ“š Coursera ê²°ê³¼:")
        print(result['coursera_result'])
        print("\nğŸ¢ Internal ê²°ê³¼:")
        print(result['internal_result'])
        print("\nğŸ† Certifications ê²°ê³¼:")
        print(result['certifications_result'])
        print("-" * 50)
        
        return result
    
    # 2. ìµœì¢… ê²°ê³¼ê°’ë§Œ ë³´ì—¬ì£¼ëŠ” í…ŒìŠ¤íŠ¸
    async def test_final_result():
        print("\n=== ìµœì¢… ê²°ê³¼ í…ŒìŠ¤íŠ¸ ===")
        query = "í´ë¼ìš°ë“œ ì–´ë–»ê²Œ ê³µë¶€í•˜ì§€?"
        
        # ì „ì²´ ê·¸ë˜í”„ ì‹¤í–‰
        graph = create_graph()
        final_result = await graph.ainvoke({
            "messages": [HumanMessage(content=query)]
        })
        
        print("ğŸ¯ ìµœì¢… ì¶”ì²œ ê²°ê³¼:")
        print(final_result["messages"][-1].content)
    
    # ì‹¤í–‰
    async def run_tests():
        # ê° ë…¸ë“œ ê²°ê³¼ í™•ì¸
        # await test_each_node()
        
        # ìµœì¢… ê²°ê³¼ í™•ì¸
        await test_final_result()
    
    asyncio.run(run_tests())