from agents.main_chatbot.prompt import exception_prompt, intent_prompt, rewrite_prompt, rag_prompt, \
                                        keyword_prompt, chat_summary_prompt, trend_prompt
from agents.main_chatbot.prompt import similar_analysis_prompt, career_recommend_prompt, \
                                       tech_extraction_prompt, future_search_prompt, future_job_prompt,integration_prompt,\
                                       internal_expert_mento_prompt, search_keyword_prompt, external_expert_mento_prompt
from agents.main_chatbot.developstate import DevelopState
from agents.main_chatbot.config import MODEL_NAME, TEMPERATURE, role, skill_set, domain, job
from agents.main_chatbot.response import PromptWrite, PathRecommendResult, RoleModelGroup, GroupedRoleModelResult, SimilarRoadMapResult,TrendResult
from db.postgres import get_company_direction

from langchain_core.prompts import PromptTemplate
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser, PydanticOutputParser
from langchain_openai import ChatOpenAI

from vector_store.chroma_search import find_best_match, get_topN_info
from agents.tools.trend_search import trend_analysis_for_keywords, parse_keywords, format_search_results, tavily_search_for_keywords, trend_search
from agents.tools.tavily_search import search_tavily
from agents.tools.lecture_search import lecture_recommend
import json, asyncio

from concurrent.futures import ThreadPoolExecutor
import time

# monitoring
from agents.main_chatbot.performance_monitor import performance_tracker


# pydantic error -> ë°˜ë³µ ì‹¤í–‰ íšŸìˆ˜ ì„¤ì •
def limited_retry_chain(chain, input_data: dict, max_retries: int = 2):
    for attempt in range(1, max_retries + 1):
        try:
            return chain.invoke(input_data)
        except Exception as e:
            print(f"âš ï¸ Retry {attempt} failed: {e}")
            if attempt == max_retries:
                raise


def clean_brackets(text_or_list):
    """ëŒ€ê´„í˜¸ ì œê±° í—¬í¼ í•¨ìˆ˜"""
    if isinstance(text_or_list, list):
        return [item.replace('[', '').replace(']', '') for item in text_or_list]
    elif isinstance(text_or_list, str):
        return text_or_list.replace('[', '').replace(']', '')
    return text_or_list


def exception(state: DevelopState) -> DevelopState:    
    ex_prompt = PromptTemplate(
                    input_variables=["query", "job", "role", "skill_set", "domain"],
                    template=exception_prompt
                    )
    llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
    exception_chain = ex_prompt | llm | StrOutputParser()

    result = exception_chain.invoke({
        "query": state.get('input_query'),
        "job": job,
        "role": role,
        "skill_set": skill_set,
        "domain": domain
    })
    return {
        **state,
        'result': {'text' : result
                   },
        'messages': AIMessage(content=result, name="EXCEPTION")
    }


@performance_tracker("Intent ë¶„ì„")
def intent_analize(state: DevelopState) -> DevelopState:
    input_query = state['input_query']
    agents=["path_recommend", "role_model", "trend_path", "career_goal", "EXCEPTION"]
    prompt = intent_prompt.format(agents=agents)
    messages = [AIMessage(content=prompt), HumanMessage(input_query)]
    llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
    response = llm.invoke(messages)
    intent = response.content.strip()
    print(f"Intent: {intent}")
    return {**state, 
            'intent': intent,
            'messages': HumanMessage(input_query)}


@performance_tracker("ì¿¼ë¦¬ ì¬ì‘ì„±")
def rewrite(state: DevelopState) -> DevelopState:
    try:
        input_query = state.get("input_query", "")
        user_id = state.get("user_id", "")
        career_summary = state['career_summary']
        intent = state['intent']
        chat_summary = state['chat_summary']
        if not input_query:
            return {**state, 
                    'result': {'detail': 'ì…ë ¥ëœ ì§ˆì˜ê°€ ì—†ìŠµë‹ˆë‹¤.'}}
        if not user_id:
            return {**state, 
                    "result": {'detail': 'ì‚¬ìš©ì IDê°€ ì—†ìŠµë‹ˆë‹¤.'}}
        rewriter_prompt = PromptTemplate(
            input_variables=["career_summary", "chat_summary", "user_query", "role", "skill_set", "domain", "intent"], 
            template=rewrite_prompt
            )
        llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
        rewriter_chain = rewriter_prompt | llm.with_structured_output(PromptWrite)
        rewritten_result = rewriter_chain.invoke({
            "career_summary": career_summary,
            "chat_summary" : chat_summary,
            "user_query": input_query,
            "intent": intent,
            # "direction": direction,
            "role": role, "skill_set": skill_set, "domain": domain
        })
        return {
            **state, 
            'rewrited_query': rewritten_result.new_query
        }
    except Exception as e:
        return {**state, 
                'result': {'detail': f"ì¿¼ë¦¬ ì¬ì‘ì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"}}


@performance_tracker("RAG ì¿¼ë¦¬ ì¬ìƒì„±")
def ragwrite(state: DevelopState) -> DevelopState:
    """RAG ê³¼ì •ì—ì„œ ì‚¬ìš©í•  ì§ˆì˜ ì¬ìƒì„± ë…¸ë“œ (chromaDB version)"""
    query = state.get('rewrited_query', '')
    intent = state.get('intent', '')
    prompt=[HumanMessage(content=rag_prompt.format(query=query, intent=intent))]
    llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
    response = llm.invoke(prompt)
    return {**state, 
            'rag_query': response.content.strip()  
            }


def similar_roadmap(state:DevelopState) -> DevelopState:
    """
    ì¶”ì¶œëœ ì‚¬ë‚´ êµ¬ì„±ì› ë°ì´í„° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³µí†µì  ë¶„ì„í•˜ì—¬ ë¡œë“œë§µ ì‘ì„±í•˜ëŠ” ë…¸ë“œ
    """
    try: 
        info = find_best_match(state.get('rag_query', ''), state.get('user_id', ''))
        similar_roadmap_prompt = PromptTemplate(
            input_variables=["internal_employee", "career_summary", "user_query", "role", "job", "skill_set"],
            template=similar_analysis_prompt
        )
        parser = PydanticOutputParser(pydantic_object=SimilarRoadMapResult)
        llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE, verbose=True)

        similar_chain = similar_roadmap_prompt | llm | parser
        try:
            result = limited_retry_chain(similar_chain, {
                "internal_employee": info,
                "career_summary": state.get("career_summary", ""),
                "user_query": state.get("rewrited_query", ""),
                "role": role, "skill_set": skill_set, "job": job
            }, max_retries=2)

        except Exception as e:
            print("âŒ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨. ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜.", e)
            result = SimilarRoadMapResult()

    except Exception as e:
        print("âŒ ì§„í–‰ ì¤‘ ì‹¤íŒ¨. ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜.", e)
        result = SimilarRoadMapResult()
    
    # print(result)
    return {**state,
            'result': result.to_output_dict(),
            'messages': [AIMessage(result.similar_analysis_text),
                         AIMessage(json.dumps(result.to_output_dict()["similar_roadmaps"], ensure_ascii=False))
                        ]
        }

def path(state: DevelopState) -> DevelopState:
    """ 
    ì¶”ì¶œëœ ì‚¬ë‚´ êµ¬ì„±ì› ë°ì´í„° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²½ë ¥ ì¦ì§„ ê²½ë¡œ ì¶”ì²œí•˜ëŠ” ë…¸ë“œ
    í˜„ì¬: ëª…ì‹œì ìœ¼ë¡œ ë‚´ë¶€ì—ì„œ ë…¸ë“œ ìˆœì°¨ì  ì‹¤í–‰í•˜ê²Œ ë§Œë“¤ì–´ ë‘ 
    """
    try:
        # info = find_best_match(state.get('rag_query',''),state.get('user_id',''))
        direction = get_company_direction()
        path_recommend_prompt = PromptTemplate(
        input_variables=["internal_employee", "career_summary", "user_query", "role", "job", "skill_set", "direction"],
        template=career_recommend_prompt
        )
        llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE, verbose=True)

        parser = PydanticOutputParser(pydantic_object=PathRecommendResult)
        path_chain = path_recommend_prompt | llm | parser
        try:
            result = limited_retry_chain(path_chain, {
                "career_summary": state.get("career_summary", ""),
                "user_query": state.get("rewrited_query", ""),
                "common_patterns": state.get("result", {}).get("similar_roadmaps", ""),
                "direction": direction,
                "role": role, "skill_set": skill_set, "job": job
            }, max_retries=2)

        except Exception as e:
            print("âŒ ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨. ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜.", e)
            result = PathRecommendResult()

    except Exception as e:
        print("âŒ ì§„í–‰ ì¤‘ ì‹¤íŒ¨. ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜.", e)
        result = PathRecommendResult()

    # print(result)
    return {**state,
            'result': {**state.get("result", {}),
                        **result.to_output_dict()
                      },
            'messages': [AIMessage(result.career_path_text),
                         AIMessage(json.dumps(
                                [r.model_dump() for r in result.career_path_roadmap],
                                ensure_ascii=False))]
            }
    


### role_model ìƒì„± ê´€ë ¨ ë…¸ë“œ (ë¹„ë™ê¸°)
# ë¹„ë™ê¸° ì²˜ë¦¬
async def run_in_thread(func, *args, **kwargs):
    """ë™ê¸° í•¨ìˆ˜ë¥¼ ìŠ¤ë ˆë“œì—ì„œ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰"""
    loop = asyncio.get_event_loop()
    with ThreadPoolExecutor() as executor:
        return await loop.run_in_executor(executor, func, *args, **kwargs)


# ë¹„ë™ê¸° DB ì¡°íšŒ í•¨ìˆ˜
async def get_topN_info_async(query_text, user_id, top_n, grade=None, years=False):
    """ë¹„ë™ê¸° DB ì¡°íšŒ"""
    print(f"ğŸ” DB ì¡°íšŒ ì‹œì‘: {query_text[:30]}...")
    start_time = time.time()
    
    # ë™ê¸° í•¨ìˆ˜ë¥¼ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
    result = await run_in_thread(get_topN_info, query_text, user_id, top_n, grade, years)
    
    duration = time.time() - start_time
    print(f"âœ… DB ì¡°íšŒ ì™„ë£Œ: {duration:.2f}ì´ˆ")
    return result


async def gpt_generate_with_parser_async(prompt_template, input_vars, parser=None, max_retries=2):
   """GPT API ë¹„ë™ê¸° í˜¸ì¶œ + íŒŒì‹±"""
   for attempt in range(max_retries + 1):
       try:
           # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ…
           formatted_prompt = prompt_template.format(**input_vars)
           
           # GPT APIë¥¼ ìŠ¤ë ˆë“œì—ì„œ ë¹„ë™ê¸°ë¡œ í˜¸ì¶œ
           def call_gpt():
               llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
               
               if parser:
                   # íŒŒì„œê°€ ìˆìœ¼ë©´ ì²´ì¸ ì‚¬ìš©
                   chain = prompt_template | llm | parser
                   result = chain.invoke(input_vars)
                   return result
               else:
                   # íŒŒì„œê°€ ì—†ìœ¼ë©´ ë‹¨ìˆœ í…ìŠ¤íŠ¸ í˜¸ì¶œ
                   response = llm.invoke([{"role": "user", "content": formatted_prompt}])
                   return response.content.strip()
           
           api_start = time.time()
           result = await run_in_thread(call_gpt)
           api_duration = time.time() - api_start
           print(f"  ğŸ“¡ GPT API í˜¸ì¶œ: {api_duration:.2f}ì´ˆ")
           
           return result
               
       except Exception as e:
           if attempt < max_retries:
               print(f"âš ï¸ Retry {attempt + 1} failed: {e}")
               await asyncio.sleep(1)
           else:
               raise e

     
@performance_tracker("ì‚¬ë‚´ ì „ë¬¸ê°€ ë©˜í†  ìƒì„±")
async def create_internal_expert(state: DevelopState) -> DevelopState:
   """ë¹„ë™ê¸° ì‚¬ë‚´ ì „ë¬¸ê°€ ë©˜í†  ìƒì„±"""
   try:
       print('ğŸ”¥ ì‚¬ë‚´ ì „ë¬¸ê°€ ë©˜í†  ìƒì„± ì‹œì‘ (ë¹„ë™ê¸°)')
       start_time = time.time()
       
       user_query = state.get('rag_query', state.get('input_query', ''))
       print(user_query)
       
       # 1. ë¹„ë™ê¸° DB ì¡°íšŒ
       db_start = time.time()
       expert_info = await get_topN_info_async(
           query_text=user_query,
           user_id=state.get('user_id', ''),
           grade='CL4',
           top_n=5
       )
       db_duration = time.time() - db_start
       print(f"  ğŸ—„ï¸ DB ì¡°íšŒ: {db_duration:.2f}ì´ˆ")
       
       if not expert_info.strip():
           return None
               
       # 2. í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
       similar_prompt = PromptTemplate(
           input_variables=["user_query", "internal_employees", "total_count", "skill_set", "role", "job", "domain"],
           template=internal_expert_mento_prompt
       )
       parser = PydanticOutputParser(pydantic_object=RoleModelGroup)
       
       # 3. ë¹„ë™ê¸° GPT í˜¸ì¶œ
       gpt_start = time.time()
       result = await gpt_generate_with_parser_async(
           similar_prompt,
           {
               "user_query": user_query,
               "internal_employees": expert_info,
               "total_count": 5,
               "skill_set": skill_set,
               "role": role,
               "domain": domain,
               "job": job
           },
           parser,
           max_retries=2
       )
       gpt_duration = time.time() - gpt_start
       print(f"  ğŸ¤– GPT ì²˜ë¦¬: {gpt_duration:.2f}ì´ˆ")
       
    #    print(result)
       result.group_name = "ì‚¬ë‚´ ì „ë¬¸ê°€"
       result.common_project = clean_brackets(result.common_project)

       total_duration = time.time() - start_time
       print(f"âœ… ì‚¬ë‚´ ì „ë¬¸ê°€ ë©˜í†  ìƒì„± ì™„ë£Œ: {total_duration:.2f}ì´ˆ")
       return result
   
   except Exception as e:
       print(f"âŒ ì‚¬ë‚´ ì „ë¬¸ê°€ ë©˜í†  ìƒì„± ì‹¤íŒ¨: {e}")
       return None


@performance_tracker("ì‚¬ë‚´ ìœ ì‚¬ ê²½ë ¥ ë©˜í†  ìƒì„±")
async def create_internal_similar(state: DevelopState) -> DevelopState:
   """ë¹„ë™ê¸° ì‚¬ë‚´ ìœ ì‚¬ ê²½ë ¥ ë©˜í†  ìƒì„±"""
   try:
       print('ğŸ”¥ ì‚¬ë‚´ ìœ ì‚¬ ê²½ë ¥ ë©˜í†  ìƒì„± ì‹œì‘ (ë¹„ë™ê¸°)')
       start_time = time.time()
       
       user_query = state.get('rag_query', state.get('input_query', ''))
       
       # 1. ë¹„ë™ê¸° DB ì¡°íšŒ
       similar_info = await get_topN_info_async(
           query_text=user_query,
           user_id=state.get('user_id', ''),
           years=True,
           top_n=5
       )
       
       if not similar_info.strip():
           return None
               
       # 2. í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
       similar_prompt = PromptTemplate(
           input_variables=["user_query", "internal_employees", "total_count", "skill_set", "role", "job", "domain"],
           template=internal_expert_mento_prompt
       )
       parser = PydanticOutputParser(pydantic_object=RoleModelGroup)
       
       # 3. ë¹„ë™ê¸° GPT í˜¸ì¶œ
       result = await gpt_generate_with_parser_async(
           similar_prompt,
           {
               "user_query": user_query,
               "internal_employees": similar_info,
               "total_count": 5,
               "skill_set": skill_set,
               "role": role,
               "domain": domain,
               "job": job
           },
           parser,
           max_retries=2
       )
       
       result.group_name = "ì‚¬ë‚´ ìœ ì‚¬ ê²½ë ¥ êµ¬ì„±ì›"
       result.common_project = clean_brackets(result.common_project)

    #    print(result)
       total_duration = time.time() - start_time
       print(f"âœ… ì‚¬ë‚´ ìœ ì‚¬ ê²½ë ¥ ë©˜í†  ìƒì„± ì™„ë£Œ: {total_duration:.2f}ì´ˆ")
       return result
   
   except Exception as e:
       print(f"âŒ ì‚¬ë‚´ ìœ ì‚¬ ê²½ë ¥ ë©˜í†  ìƒì„± ì‹¤íŒ¨: {e}")
       return None


@performance_tracker("ì™¸ë¶€ ì „ë¬¸ê°€ ë©˜í†  ìƒì„±")
async def create_external_expert(state: DevelopState) -> DevelopState:
   """ë¹„ë™ê¸° ì™¸ë¶€ ì „ë¬¸ê°€ ë©˜í†  ìƒì„±"""
   try:
       print('ğŸ”¥ ì™¸ë¶€ ì „ë¬¸ê°€ ë©˜í†  ìƒì„± ì‹œì‘ (ë¹„ë™ê¸°)')
       start_time = time.time()
       
       user_query = state.get('rag_query', state.get('input_query', ''))
       
       # 1. ë¹„ë™ê¸° í‚¤ì›Œë“œ ìƒì„±
       keyword_prompt = PromptTemplate(
           input_variables=["user_query"],
           template=search_keyword_prompt
       )
       
       keyword_start = time.time()
       keywords_text = await gpt_generate_with_parser_async(keyword_prompt, {"user_query": user_query})
       keyword_duration = time.time() - keyword_start
       print(f"  ğŸ” í‚¤ì›Œë“œ ìƒì„±: {keyword_duration:.2f}ì´ˆ")
       
       if not keywords_text.strip():
           raise ValueError("í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨")

       keywords = keywords_text.strip().split(',')

       # 2. ë³‘ë ¬ ê²€ìƒ‰ ì‹¤í–‰ (ë¹„ë™ê¸°)
       search_start = time.time()
       search_results_list = await tavily_search_for_keywords(keywords)
       search_duration = time.time() - search_start
       print(f"  ğŸŒ ì›¹ ê²€ìƒ‰: {search_duration:.2f}ì´ˆ")

       # 3. ê²€ìƒ‰ ê²°ê³¼ í†µí•©
       all_results = []
       for i, results in enumerate(search_results_list):
           if isinstance(results, Exception):
               print(f"âŒ í‚¤ì›Œë“œ {i + 1} ê²€ìƒ‰ ì‹¤íŒ¨: {results}")
               continue
           all_results.extend(results)

       if all_results:
           external_info = f"""[ì™¸ë¶€ ì „ë¬¸ê°€ ê²€ìƒ‰ ê²°ê³¼]
ê²€ìƒ‰ í‚¤ì›Œë“œ: {', '.join(keywords[:5])}
ì‚¬ìš©ì ì§ˆë¬¸: {user_query}

=== ê²€ìƒ‰ ê²°ê³¼ ===
{chr(10).join(all_results)}
"""
       else:
           external_info = f"""[ì™¸ë¶€ ì „ë¬¸ê°€ ê¸°ë³¸ ì •ë³´]
ì‚¬ìš©ì ì§ˆë¬¸: {user_query}
ê²€ìƒ‰ ê²°ê³¼ê°€ ì œí•œì ì´ë¯€ë¡œ ê¸°ë³¸ ì •ë³´ë¡œ ë©˜í†  ìƒì„± í•„ìš”
"""

       # 4. ë¹„ë™ê¸° ì™¸ë¶€ ì „ë¬¸ê°€ ë©˜í†  ìƒì„±
       external_prompt = PromptTemplate(
           input_variables=["user_query", "skill_set", "domain", "job", "external_info"],
           template=external_expert_mento_prompt
       )
       parser = PydanticOutputParser(pydantic_object=RoleModelGroup)

       gpt_start = time.time()
       result = await gpt_generate_with_parser_async(
           external_prompt,
           {
               "user_query": user_query,
               "skill_set": skill_set,
               "domain": domain,
               "job": job,
               "external_info": external_info
           },
           parser,
           max_retries=2
       )
       gpt_duration = time.time() - gpt_start
       print(f"  ğŸ¤– GPT ì²˜ë¦¬: {gpt_duration:.2f}ì´ˆ")
       
       result.group_name = "ì™¸ë¶€ ì „ë¬¸ê°€"
       result.real_info = []
       
       total_duration = time.time() - start_time
       print(f"âœ… ì™¸ë¶€ ì „ë¬¸ê°€ ë©˜í†  ìƒì„± ì™„ë£Œ: {total_duration:.2f}ì´ˆ")
       return result

   except Exception as e:
       print(f"âŒ ì™¸ë¶€ ì „ë¬¸ê°€ ë©˜í†  ìƒì„± ì‹¤íŒ¨: {e}")
       return None

@performance_tracker("ë¡¤ëª¨ë¸ ìƒì„±")
async def role_model(state: DevelopState) -> DevelopState:
   """
   ë¡¤ëª¨ë¸ ê·¸ë£¹ ìƒì„± (ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬)
   Ver.4 ì™„ì „í•œ ë¹„ë™ê¸° ë³‘ë ¬ ìƒì„±ìœ¼ë¡œ ì„±ëŠ¥ ìµœì í™”
   """
   try:
       # ëª¨ë“  íƒœìŠ¤í¬ë¥¼ ë³‘ë ¬ë¡œ ì‹¤í–‰
       print("ğŸš€ ë©˜í†  ìƒì„± íƒœìŠ¤í¬ë“¤ì„ ë³‘ë ¬ë¡œ ì‹œì‘... (GPT ë¹„ë™ê¸°)")
       parallel_start = time.time()
       
       results = await asyncio.gather(
           create_internal_expert(state),
           create_internal_similar(state), 
           create_external_expert(state),
           return_exceptions=True
       )
       
       parallel_duration = time.time() - parallel_start
       print(f"ğŸ”„ ë³‘ë ¬ ì‹¤í–‰ ì™„ë£Œ: {parallel_duration:.2f}ì´ˆ")
       
       # ì„±ê³µí•œ ê²°ê³¼ë§Œ ìˆ˜ì§‘
       successful_groups = []
       for i, result in enumerate(results):
           if isinstance(result, Exception):
               print(f"âŒ íƒœìŠ¤í¬ {i+1} ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {result}")
               continue
           if result is not None:
               successful_groups.append(result)
       
       # ìµœì¢… ê²°ê³¼ êµ¬ì„±
       if successful_groups:
           final_result = GroupedRoleModelResult(
               analysis_summary=f"ì´ {len(successful_groups)}ê°œì˜ ë©˜í†  ê·¸ë£¹ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.",
               groups=successful_groups
           )
       else:
           print("âŒ ìƒì„±ëœ ë©˜í†  ê·¸ë£¹ì´ ì—†ìŠµë‹ˆë‹¤.")
           final_result = GroupedRoleModelResult()
       
       # ê¸°ì¡´ í¬ë§·ìœ¼ë¡œ ë³€í™˜
       role_model_list = []
       for i, group in enumerate(final_result.groups):
           role_model_dict = {
               'group_id': f'group_{i+1}',
               'group_name': group.group_name,
               'current_position': group.role_model.current_position,
               'experience_years': group.role_model.experience_years,
               'main_domains': group.role_model.main_domains,
               'advice_message': group.role_model.advice_message,
               'common_skill_set': group.common_skill_set,
               'common_career_path': group.common_career_path,
               'common_project': group.common_project,
               'common_experience': group.common_experience,
               'common_cert': group.common_cert
           }
           role_model_list.append(role_model_dict)
       
       # ìš”ì•½ í…ìŠ¤íŠ¸ ìƒì„±
       if role_model_list:
           summary_text = f"""ğŸ¯ ì§„ì§œ ë³‘ë ¬ ë©˜í†  ìƒì„± ì™„ë£Œ (GPT ë¹„ë™ê¸°)

ğŸ“‹ ìƒì„±ëœ ë¡¤ëª¨ë¸ ê·¸ë£¹ ({len(role_model_list)}ê°œ):
{chr(10).join([f"â€¢ {group.group_name}: {group.role_model.name}" for group in final_result.groups])}

ğŸ’¡ {final_result.analysis_summary}
âš¡ ë³‘ë ¬ ì‹¤í–‰ ì‹œê°„: {parallel_duration:.2f}ì´ˆ

ğŸ“Š ìƒì„¸ ë¡¤ëª¨ë¸ ì •ë³´:
{chr(10).join([
   f"â–¶ {model['group_name']} (ID: {model['group_id']})" + chr(10) +
   f"  â€¢ í˜„ì¬ ì§ì±…: {model['current_position']}" + chr(10) +
   f"  â€¢ ê²½ë ¥: {model['experience_years']}" + chr(10) +
   f"  â€¢ ì£¼ìš” ë„ë©”ì¸: {', '.join(model['main_domains'])}" + chr(10) +
   f"  â€¢ ì¡°ì–¸: {model['advice_message']}" + chr(10) +
   f"  â€¢ ê³µí†µ ê¸°ìˆ  ìŠ¤íƒ: {', '.join(model['common_skill_set'])}" + chr(10) +
   f"  â€¢ ê³µí†µ ê²½ë ¥ ì¦ì§„ íŒ¨í„´: {', '.join(model['common_career_path'])}" + chr(10) +
   f"  â€¢ ê³µí†µ í”„ë¡œì íŠ¸: {', '.join(model['common_project'])}" + chr(10) +
   f"  â€¢ ê³µí†µ ê²½í—˜: {', '.join(model['common_experience'])}" + chr(10) +
   f"  â€¢ ê³µí†µ ìê²©ì¦: {', '.join(model['common_cert'])}" + chr(10)
   for model in role_model_list
])}
"""
       else:
           summary_text = "âŒ ë©˜í†  ê·¸ë£¹ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
           
   except Exception as e:
       print(f"âŒ ë¡¤ëª¨ë¸ ìƒì„± ì¤‘ ì „ì²´ ì˜¤ë¥˜ ë°œìƒ: {e}")
       return {**state, 'error': f'ë¡¤ëª¨ë¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}

   return {
       **state,
       'result': {
           'rolemodels': role_model_list,
       },
       'messages': AIMessage(summary_text)
   }


async def trend(state: DevelopState) -> DevelopState:
    """
    ê¸°ìˆ  íŠ¸ë Œë“œ ê²€ìƒ‰ê³¼ ì‚¬ë‚´ êµìœ¡ ì¶”ì²œì„ í†µí•©í•˜ëŠ” ìµœì¢… í•¨ìˆ˜
    """
    try:
        llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
        user_query = state.get('input_query')
        career_summary = state.get("career_summary")

        
        # 1. ë³‘ë ¬ë¡œ ê¸°ìˆ  íŠ¸ë Œë“œ ê²€ìƒ‰ê³¼ ì‚¬ë‚´ êµìœ¡ ì¶”ì²œ ì‹¤í–‰
        trend_analysis, lecture_recommendation = await asyncio.gather(
            trend_search(user_query),             
            lecture_recommend(user_query,career_summary)     
        )

        trend_result = trend_analysis.get('trend_result', '')
        internal_course = lecture_recommendation.get('internal_course', '')
        ax_college = lecture_recommendation.get('ax_college', '')
        explanation = lecture_recommendation.get('explanation', '')
        
        # 4. í†µí•© ë¶„ì„ ì‹¤í–‰
        integration_template = PromptTemplate(
            input_variables=["career_summary", "trend_result", "internal_course","ax_college", "explanation"],
            template=integration_prompt
        )
        
        integration_chain = integration_template | llm
        final_result = integration_chain.invoke({
            "career_summary": state.get("career_summary"),
            "trend_result": trend_result,
            "internal_course": internal_course,
            "ax_college": ax_college,
            "explanation": explanation
        })
        
        return {
            **state,
            'result': {'text': final_result.content,
                       'ax_college': ax_college},
            'messages': AIMessage(final_result.content)
        }
        
        
    except Exception as e:
        # ì˜¤ë¥˜ ì²˜ë¦¬
        error_message = f"í†µí•© ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        return {
            **state,
            'result': {'text': error_message,
                       'ax_college': error_message},
            'messages': AIMessage(error_message)
        }

    

async def future_career_recommend(state: DevelopState) -> DevelopState:
    """
    ê²½ë ¥ ìš”ì•½ ê¸°ë°˜ ë¯¸ë˜ ì§ë¬´ ì¶”ì²œ ë…¸ë“œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    1. ê²½ë ¥ ìš”ì•½, íšŒì‚¬ ë°©í–¥ì„±ì—ì„œ ì£¼ìš” ê¸°ìˆ  ìŠ¤íƒ íŒŒì•…
    2. ë‹¤ì¤‘ ì†ŒìŠ¤ ê²€ìƒ‰ìœ¼ë¡œ ìµœì‹  ê¸°ìˆ  íŠ¸ë Œë“œ ìˆ˜ì§‘ (ìµœëŒ€ 2íšŒ ì¬ì‹œë„)
    3. 15ë…„ í›„ ì§ë¬´ ì¶”ì²œ
    """
    
    async def search_with_retry(keywords: list, max_retries: int = 2) -> tuple[str, bool]:
        """ê²€ìƒ‰ ë° í¬ë§·íŒ…ì„ ì¬ì‹œë„í•˜ëŠ” í•¨ìˆ˜"""
        last_error = None
        
        for attempt in range(max_retries + 1):  # ìµœì´ˆ 1íšŒ + ì¬ì‹œë„ 2íšŒ = ì´ 3íšŒ
            try:
                print(f"ê²€ìƒ‰ ì‹œë„ {attempt + 1}íšŒì°¨...")
                
                # ê²€ìƒ‰ ì‹¤í–‰
                search_results = await trend_analysis_for_keywords(keywords)
                
                # ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
                if not search_results or len(search_results) == 0:
                    raise ValueError("ê²€ìƒ‰ ê²°ê³¼ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                
                # í¬ë§·íŒ… ì‹œë„
                formatted_results = format_search_results(search_results)
                
                # í¬ë§·íŒ… ê²°ê³¼ê°€ ì˜ë¯¸ìˆëŠ”ì§€ í™•ì¸
                if not formatted_results or len(formatted_results.strip()) < 50:
                    raise ValueError("í¬ë§·íŒ…ëœ ê²°ê³¼ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤")
                
                print(f"ê²€ìƒ‰ ì„±ê³µ! (ì‹œë„ {attempt + 1}íšŒì°¨)")
                return formatted_results, True  # ì„±ê³µ ì‹œ (ê²°ê³¼, True) ë°˜í™˜
                
            except Exception as e:
                last_error = e
                print(f"ê²€ìƒ‰ ì‹œë„ {attempt + 1}íšŒì°¨ ì‹¤íŒ¨: {str(e)}")
                
                if attempt < max_retries:
                    print(f"{2-attempt}íšŒ ë” ì¬ì‹œë„í•©ë‹ˆë‹¤...")
                    await asyncio.sleep(1)  # 1ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„
                else:
                    print("ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨")
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ
        return f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ì´ {max_retries + 1}íšŒ ì‹œë„): {str(last_error)}", False
    
    try:
        # íšŒì‚¬ ë°©í–¥ì„± ì£¼ì…
        # direction = 'ëª¨ë“  ì—…ë¬´ì™€ í”„ë¡œì íŠ¸ì— AIë¥¼ ê¸°ë³¸ ì ìš©í•  ì¤„ ì•„ëŠ” AI ê¸°ë³¸ ì—­ëŸ‰ì„ ê°–ì¶˜ ì‚¬ë‚´ êµ¬ì„±ì›'
        direction = get_company_direction()

        career_summary = state.get('career_summary', '')
        user_query = state.get('input_query', '')
        
        if not career_summary or career_summary == "None":
            return {
                **state,
                'result': {
                    'text': 'ê²½ë ¥ ì •ë³´ê°€ ì—†ì–´ì„œ ë¯¸ë˜ ì§ë¬´ë¥¼ ì¶”ì²œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë ¥ ì •ë³´ë¥¼ ë¨¼ì € ë“±ë¡í•´ì£¼ì„¸ìš”.'
                },
                'messages': AIMessage('ê²½ë ¥ ì •ë³´ê°€ ì—†ì–´ì„œ ë¯¸ë˜ ì§ë¬´ë¥¼ ì¶”ì²œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
            }
        
        llm = ChatOpenAI(model=MODEL_NAME, temperature=0)
        
        # 1ë‹¨ê³„: ê²½ë ¥ ìš”ì•½ì—ì„œ ê¸°ìˆ  ìŠ¤íƒ ì¶”ì¶œ
        tech_extraction_template = PromptTemplate(
            input_variables=["career_summary", "user_query", "direction"],
            template=tech_extraction_prompt
        )
        tech_extraction_chain = tech_extraction_template | llm
        tech_analysis_result = tech_extraction_chain.invoke({
            "career_summary": career_summary,
            "user_query": user_query,
            "direction": direction
        })

        # 2ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ
        keyword_template = PromptTemplate(
            input_variables=["analysis_result"],
            template=future_search_prompt
        )
        keyword_chain = keyword_template | llm
        keywords_result = keyword_chain.invoke({
            "analysis_result": tech_analysis_result.content
        })

        # 3ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ ìµœì‹  ë‚´ìš© ê²€ìƒ‰ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
        extracted_keywords = [kw.strip() for kw in keywords_result.content.split(',')]
        
        # ì¬ì‹œë„ ë¡œì§ìœ¼ë¡œ ê²€ìƒ‰ ì‹¤í–‰
        formatted_search_results, search_success = await search_with_retry(extracted_keywords, max_retries=2)
        
        # ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ íŠ¹ë³„ ë©”ì‹œì§€ ë°˜í™˜
        if not search_success:
            return {
                **state,
                'result': {
                    'text': 'ê²€ìƒ‰ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë” ë§ì€ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì§ˆì˜ë¥¼ ë³´ë‚´ì£¼ì„¸ìš”.'
                },
                'messages': AIMessage('ê²€ìƒ‰ëœ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë” ë§ì€ ì •ë³´ë¥¼ í¬í•¨í•˜ì—¬ ì§ˆì˜ë¥¼ ë³´ë‚´ì£¼ì„¸ìš”.')
            }

        # 4ë‹¨ê³„: 15ë…„ í›„ ë¯¸ë˜ ì§ë¬´ ì¶”ì²œ
        future_job_template = PromptTemplate(
            input_variables=["career_summary", "tech_analysis", "search_tech_trends", "direction"],
            template=future_job_prompt
        )
        future_job_chain = future_job_template | llm
        final_result = future_job_chain.invoke({
            "career_summary": career_summary,
            "tech_analysis": tech_analysis_result.content,
            "search_tech_trends": formatted_search_results,
            "direction": direction
        })

        return {
            **state,
            'result': {
                'text': final_result.content,
            },
            'messages': AIMessage(final_result.content)
        }
        
    except Exception as e:
        return {
            **state,
            'result': {
                'text': f'ë¯¸ë˜ ì§ë¬´ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
            },
            'error': f'ë¯¸ë˜ ì§ë¬´ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
        }
    
    
def chat_summary(state: DevelopState) -> DevelopState:
    llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
    chat_summary_prompttamplate = PromptTemplate(
    input_variables=["chat_summary","user_question","answer"],
    template=chat_summary_prompt
    )
    chat_summary_llm_chain = chat_summary_prompttamplate | llm
    result = chat_summary_llm_chain.invoke({
        "chat_summary": state.get('chat_summary'),
        "user_question": state.get('input_query'),
        "answer": state.get('result')
    })
    return {**state,
        'chat_summary': result.content,
        'messages': AIMessage(result.content)}
