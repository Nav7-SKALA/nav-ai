from agents.main_chatbot.prompt import exception_prompt, intent_prompt, rewrite_prompt, rag_prompt, \
                                        role_prompt, keyword_prompt, chat_summary_prompt
from agents.main_chatbot.prompt import similar_analysis_prompt, career_recommend_prompt, \
                                       tech_extraction_prompt, future_search_prompt, future_job_prompt, integration_prompt
from agents.main_chatbot.developstate import DevelopState
from agents.main_chatbot.config import MODEL_NAME, TEMPERATURE, role, skill_set, domain, job
from agents.main_chatbot.response import PromptWrite, PathRecommendResult, GroupedRoleModelResult, SimilarRoadMapResult, \
                                        TrendResult
from db.postgres import get_company_direction

from langchain_core.prompts import PromptTemplate
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI
from vector_store.chroma_search import find_best_match, get_topN_info, get_topN_emp
from agents.tools.trend_search import trend_analysis_for_keywords, parse_keywords, format_search_results,trend_search
from agents.tools.lecture_search import lecture_recommend
import json, asyncio


def exception(state: DevelopState) -> DevelopState:    
    ex_prompt = PromptTemplate(
                    input_variables=["query"],
                    template=exception_prompt
                    )
    llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
    exception_chain = ex_prompt | llm | StrOutputParser()

    result = exception_chain.invoke({
        "query": state.get('input_query')
    })
    return {
        **state,
        'result': {'text' : result
                   },
        'messages': AIMessage(content=result, name="EXCEPTION")
    }

def intent_analize(state: DevelopState) -> DevelopState:
    input_query = state['input_query']
    agents=["path_recommend", "role_model", "trend_path", "career_goal", "EXCEPTION"]
    prompt = intent_prompt.format(agents=agents)
    messages = [AIMessage(content=prompt), HumanMessage(input_query)]
    llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
    response = llm.invoke(messages)
    intent = response.content.strip()
    print(f"Intent: {intent}")
    return {**state, 
            'intent': intent,
            'messages': HumanMessage(input_query)}

def rewrite(state: DevelopState) -> DevelopState:
    try:
        input_query = state.get("input_query", "")
        user_id = state.get("user_id", "")
        career_summary = state['career_summary']
        intent = state['intent']
        chat_summary = state['chat_summary']
        if not input_query:
            return {**state, 
                    'result': {'detail': 'ìž…ë ¥ëœ ì§ˆì˜ê°€ ì—†ìŠµë‹ˆë‹¤.'}}
        if not user_id:
            return {**state, 
                    "result": {'detail': 'ì‚¬ìš©ìž IDê°€ ì—†ìŠµë‹ˆë‹¤.'}}
        rewriter_prompt = PromptTemplate(
            input_variables=["career_summary", "chat_summary", "user_query", "role", "skill_set", "domain", "intent"], 
            template=rewrite_prompt
            )
        llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
        rewriter_chain = rewriter_prompt | llm.with_structured_output(PromptWrite)
        rewritten_result = rewriter_chain.invoke({
            "career_summary": career_summary,
            "chat_summary" : chat_summary,
            "user_query": input_query,
            "intent": intent,
            # "direction": direction,
            "role": role, "skill_set": skill_set, "domain": domain
        })
        return {
            **state, 
            'rewrited_query': rewritten_result.new_query
        }
    except Exception as e:
        return {**state, 
                'result': {'detail': f"ì¿¼ë¦¬ ìž¬ìž‘ì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"}}

def ragwrite(state: DevelopState) -> DevelopState:
    """RAG ê³¼ì •ì—ì„œ ì‚¬ìš©í•  ì§ˆì˜ ìž¬ìƒì„± ë…¸ë“œ (chromaDB version)"""
    query = state.get('rewrited_query', '')
    intent = state.get('intent', '')
    prompt=[HumanMessage(content=rag_prompt.format(query=query, intent=intent))]
    llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
    response = llm.invoke(prompt)
    return {**state, 
            'rag_query': response.content.strip()  
            }

def similar_roadmap(state:DevelopState) -> DevelopState:
    """
    ì¶”ì¶œëœ ì‚¬ë‚´ êµ¬ì„±ì› ë°ì´í„° ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê³µí†µì  ë¶„ì„í•˜ì—¬ ë¡œë“œë§µ ìž‘ì„±í•˜ëŠ” ë…¸ë“œ
    """
    try: 
        info = find_best_match(state.get('rag_query', ''), state.get('user_id', ''))
        similar_roadmap_prompt = PromptTemplate(
            input_variables=["internal_employee", "career_summary", "user_query", "role", "job", "skill_set"],
            template=similar_analysis_prompt
        )
        llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
        similar_chain = similar_roadmap_prompt | llm.with_structured_output(SimilarRoadMapResult)
        result = similar_chain.invoke({
            "internal_employee": info,
            "career_summary": state.get("career_summary", ""),
            "user_query": state.get("rewrited_query", ""),
            "role": role, "skill_set": skill_set, "job": job
        })
        return {
            **state,
            'result': {'similar_text': result.similar_analysis_text,
                       'similar_roadmaps': result.similar_analysis_roadmap},
            'messages': [AIMessage(result.similar_analysis_text),
                         AIMessage(json.dumps(result.similar_analysis_roadmap, ensure_ascii=False))]
        }
    except Exception as e:
        return {**state,
                'result': {'text': "invoke"+str(e),
                           'similar_roadmap': None},
                'error': f"ìœ ì‚¬í•œ ì‚¬ë‚´ êµ¬ì„±ì› ë°ì´í„° ê¸°ë°˜ ë¡œë“œë§µ ìž‘ì„± ì¤‘ ì˜¤ë¥˜: {str(e)}"
                }

def path(state: DevelopState) -> DevelopState:
    """ 
    ì¶”ì¶œëœ ì‚¬ë‚´ êµ¬ì„±ì› ë°ì´í„° ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ê²½ë ¥ ì¦ì§„ ê²½ë¡œ ì¶”ì²œí•˜ëŠ” ë…¸ë“œ
    í˜„ìž¬: ëª…ì‹œì ìœ¼ë¡œ ë‚´ë¶€ì—ì„œ ë…¸ë“œ ìˆœì°¨ì  ì‹¤í–‰í•˜ê²Œ ë§Œë“¤ì–´ ë‘ 
    """
    try:
        # info = find_best_match(state.get('rag_query',''),state.get('user_id',''))
        direction = 'ëª¨ë“  ì—…ë¬´ì™€ í”„ë¡œì íŠ¸ì— AIë¥¼ ê¸°ë³¸ ì ìš©í•  ì¤„ ì•„ëŠ” AI ê¸°ë³¸ ì—­ëŸ‰ì„ ê°–ì¶˜ ì‚¬ë‚´ êµ¬ì„±ì›' # get_company_direction()
        path_recommend_prompt = PromptTemplate(
        input_variables=["internal_employee", "career_summary", "user_query", "role", "job", "skill_set", "direction"],
        template=career_recommend_prompt
        )
        llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
        path_chain = path_recommend_prompt | llm.with_structured_output(PathRecommendResult)
        result = path_chain.invoke({
            "career_summary": state.get("career_summary", ""),
            "user_query": state.get("rewrited_query", ""),
            "common_patterns": state.get("result", {}).get("similar_roadmaps", ""),
            "direction": direction,
            "role": role, "skill_set": skill_set, "job": job
        })
        return {
            **state,
            'result': {**state.get("result", {}),
                        'text': result.career_path_text,
                        'roadmaps': result.career_path_roadmap
                        },
            'messages': [AIMessage(result.career_path_text),
                         AIMessage(json.dumps(result.career_path_roadmap, ensure_ascii=False))]
        }
    except Exception as e:
        return {**state, 
                'result': {'text': "invoke"+ str(e),
                           'roadmap' : None
                           },
                'error' : f"ê²½ë¡œ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜: {str(e)}"
                           }
    
def role_model(state: DevelopState) -> DevelopState:
    """
    ë¡¤ëª¨ë¸ ê·¸ë£¹ ìƒì„± ë…¸ë“œ
    í˜„ìž¬: ëª…ì‹œì ìœ¼ë¡œ ë‚´ë¶€ì—ì„œ ë…¸ë“œ ìˆœì°¨ì  ì‹¤í–‰í•˜ê²Œ ë§Œë“¤ì–´ ë‘ 
    """
    try:
        top_n = 5
        info = get_topN_emp(state.get('rag_query',''), state.get('user_id',''), top_n)
        grouping_prompt = PromptTemplate(
            input_variables=["similar_employees", "user_query", "total_count", "skill_set", "job", "role", "domain"],
            template=role_prompt
            )
        llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
        rolemodel_chain = grouping_prompt | llm.with_structured_output(GroupedRoleModelResult)
        structured_result = rolemodel_chain.invoke({
            "similar_employees": info,
            "user_query": state.get('input_query'),
            "total_count": top_n,
            "skill_set": skill_set,
            "role": role,
            "domain": domain,
            "job": job
        })
        
        role_model_list = []
        for i, group in enumerate(structured_result.groups):
            role_model_dict = {
                'group_id': f'group_{i+1}',
                'group_name': group.group_name,
                # 'group_description': group.group_description,
                'current_position': group.role_model.current_position,
                'experience_years': group.role_model.experience_years,
                'main_domains': group.role_model.main_domains,
                # 'skill_set': group.role_model.skill_set,
                'advice_message': group.role_model.advice_message,
                'common_skill_set': group.common_skill_set,
                'common_career_path': group.common_career_path,
                'common_project': group.common_project,
                'common_experience': group.common_experience,
                'common_cert': group.common_cert
            }
            role_model_list.append(role_model_dict)
        
        print(role_model_list)
        # ìš”ì•½ ì •ë³´ ìƒì„±
        summary_text = f"""ðŸŽ¯ ë¶„ì„ ì™„ë£Œ: {structured_result.total_employees}ëª…ì˜ ì‚¬ì› ë°ì´í„°ë¥¼ {len(structured_result.groups)}ê°œ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜

ðŸ“‹ ìƒì„±ëœ ë¡¤ëª¨ë¸ ê·¸ë£¹:
{chr(10).join([f"â€¢ {group.group_name}: {group.role_model.name} ({group.member_count}ëª…)" for group in structured_result.groups])}

ðŸ’¡ {structured_result.analysis_summary}

ðŸ“Š ìƒì„¸ ë¡¤ëª¨ë¸ ì •ë³´:
{chr(10).join([
    f"â–¶ {model['group_name']} (ID: {model['group_id']})" + chr(10) +
    f"  â€¢ í˜„ìž¬ ì§ì±…: {model['current_position']}" + chr(10) +
    f"  â€¢ ê²½ë ¥: {model['experience_years']}" + chr(10) +
    f"  â€¢ ì£¼ìš” ë„ë©”ì¸: {', '.join(model['main_domains'])}" + chr(10) +
    f"  â€¢ ì¡°ì–¸: {model['advice_message']}" + chr(10) +
    f"  â€¢ ê³µí†µ ê¸°ìˆ  ìŠ¤íƒ: {', '.join(model['common_skill_set'])}" + chr(10) +
    f"  â€¢ ê³µí†µ ê²½ë ¥ ì¦ì§„ íŒ¨í„´: {', '.join(model['common_career_path'])}" + chr(10) +
    f"  â€¢ ê³µí†µ í”„ë¡œì íŠ¸: {', '.join(model['common_project'])}" + chr(10) +
    f"  â€¢ ê³µí†µ ê²½í—˜: {', '.join(model['common_experience'])}" + chr(10) +
    f"  â€¢ ê³µí†µ ìžê²©ì¦: {', '.join(model['common_cert'])}" + chr(10)
    for model in role_model_list
])}
"""
        
        return {
            **state,
            'result': {
                'rolemodels': role_model_list,
            },
            'messages': AIMessage(summary_text)
        }
    except Exception as e:
        return {**state, 'error': f'ë¡¤ëª¨ë¸ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'}

async def trend(state: DevelopState) -> DevelopState:
    """
    ê¸°ìˆ  íŠ¸ë Œë“œ ê²€ìƒ‰ê³¼ ì‚¬ë‚´ êµìœ¡ ì¶”ì²œì„ í†µí•©í•˜ëŠ” ìµœì¢… í•¨ìˆ˜
    """
    try:
        llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
        user_query = state.get('input_query')
        career_summary = state.get("career_summary")

        
        # 1. ë³‘ë ¬ë¡œ ê¸°ìˆ  íŠ¸ë Œë“œ ê²€ìƒ‰ê³¼ ì‚¬ë‚´ êµìœ¡ ì¶”ì²œ ì‹¤í–‰
        trend_analysis, lecture_recommendation = await asyncio.gather(
            trend_search(user_query),             
            lecture_recommend(user_query,career_summary)     
        )
        print("ê²°ê³¼ í™•ì¸í•˜ê¸°")
        print("trend_search: ",trend_analysis)
        print("*"*60)
        print("lecture_recommend: ",lecture_recommendation)
        print("*"*60)

        trend_result = trend_analysis.get('trend_result', '')
        internal_course = lecture_recommendation.get('internal_course', '')
        ax_college = lecture_recommendation.get('ax_college', '')
        explanation = lecture_recommendation.get('explanation', '')
        
        # 4. í†µí•© ë¶„ì„ ì‹¤í–‰
        integration_template = PromptTemplate(
            input_variables=["career_summary", "trend_result", "internal_course","ax_college", "explanation"],
            template=integration_prompt
        )
        
        integration_chain = integration_template | llm.with_structured_output(TrendResult)
        final_result = integration_chain.invoke({
            "career_summary": state.get("career_summary"),
            "trend_result": trend_result,
            "internal_course": internal_course,
            "ax_college": ax_college,
            "explanation": explanation
        })
        
        # 5. ìµœì¢… ê²°ê³¼ ë°˜í™˜ (ê¸°ì¡´ êµ¬ì¡° ìœ ì§€)
        return {
            **state,
            'result': {'text': final_result.text,
                        'ax_college': final_result.ax_college},
            'messages': AIMessage(final_result.text)
        }
        
    except Exception as e:
        # ì˜¤ë¥˜ ì²˜ë¦¬
        error_message = f"í†µí•© ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"
        return {
            **state,
            'result': {'text': error_message,
                       'ax_college': final_result.ax_college},
            'messages': AIMessage(error_message)
        }


async def future_career_recommend(state: DevelopState) -> DevelopState:
    """
    ê²½ë ¥ ìš”ì•½ ê¸°ë°˜ ë¯¸ëž˜ ì§ë¬´ ì¶”ì²œ ë…¸ë“œ (async ë²„ì „)
    1. ê²½ë ¥ ìš”ì•½, íšŒì‚¬ ë°©í–¥ì„±ì—ì„œ ì£¼ìš” ê¸°ìˆ  ìŠ¤íƒ íŒŒì•…
    2. ë‹¤ì¤‘ ì†ŒìŠ¤ ê²€ìƒ‰ìœ¼ë¡œ ìµœì‹  ê¸°ìˆ  íŠ¸ë Œë“œ ìˆ˜ì§‘
    3. 15ë…„ í›„ ì§ë¬´ ì¶”ì²œ
    """
    try:
        # íšŒì‚¬ ë°©í–¥ì„± ì£¼ìž…
        direction = 'ëª¨ë“  ì—…ë¬´ì™€ í”„ë¡œì íŠ¸ì— AIë¥¼ ê¸°ë³¸ ì ìš©í•  ì¤„ ì•„ëŠ” AI ê¸°ë³¸ ì—­ëŸ‰ì„ ê°–ì¶˜ ì‚¬ë‚´ êµ¬ì„±ì›' # get_company_direction()

        career_summary = state.get('career_summary', '')
        user_query = state.get('input_query', '')
        
        if not career_summary or career_summary == "None":
            return {
                **state,
                'result': {
                    'text': 'ê²½ë ¥ ì •ë³´ê°€ ì—†ì–´ì„œ ë¯¸ëž˜ ì§ë¬´ë¥¼ ì¶”ì²œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë ¥ ì •ë³´ë¥¼ ë¨¼ì € ë“±ë¡í•´ì£¼ì„¸ìš”.'
                },
                'messages': AIMessage('ê²½ë ¥ ì •ë³´ê°€ ì—†ì–´ì„œ ë¯¸ëž˜ ì§ë¬´ë¥¼ ì¶”ì²œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.')
            }
        
        llm = ChatOpenAI(model=MODEL_NAME, temperature=0)
        
        # 1ë‹¨ê³„: ê²½ë ¥ ìš”ì•½ì—ì„œ ê¸°ìˆ  ìŠ¤íƒ ì¶”ì¶œ
        tech_extraction_template = PromptTemplate(
            input_variables=["career_summary", "user_query", "direction"],
            template=tech_extraction_prompt
        )
        tech_extraction_chain = tech_extraction_template | llm
        tech_analysis_result = tech_extraction_chain.invoke({
            "career_summary": career_summary,
            "user_query": user_query,
            "direction": direction
        })

        # 2ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ
        keyword_template = PromptTemplate(
            input_variables=["analysis_result"],
            template=future_search_prompt
        )
        keyword_chain = keyword_template | llm
        keywords_result = keyword_chain.invoke({
            "analysis_result": tech_analysis_result.content
        })

        # 3ë‹¨ê³„: í‚¤ì›Œë“œ ê¸°ë°˜ ìµœì‹  ë‚´ìš© ê²€ìƒ‰
        # í‚¤ì›Œë“œë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
        extracted_keywords = [kw.strip() for kw in keywords_result.content.split(',')]
        
        # 3ë‹¨ê³„: ë‹¤ì¤‘ ì†ŒìŠ¤ ë³‘ë ¬ ê²€ìƒ‰ (GitHub, Reddit, Tavily)
        search_results = await trend_analysis_for_keywords(extracted_keywords)
        
        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…
        formatted_search_results = format_search_results(search_results)

        # 4ë‹¨ê³„: 15ë…„ í›„ ë¯¸ëž˜ ì§ë¬´ ì¶”ì²œ
        future_job_template = PromptTemplate(
            input_variables=["career_summary", "tech_analysis", "search_tech_trends", "direction"],
            template=future_job_prompt
        )
        future_job_chain = future_job_template | llm
        final_result = future_job_chain.invoke({
            "career_summary": career_summary,
            "tech_analysis": tech_analysis_result.content,
            "search_tech_trends": formatted_search_results,
            "direction": direction
        })

        return {
            **state,
            'result': {
                'text': final_result.content,
            },
            'messages': AIMessage(final_result.content)
        }
        
    except Exception as e:
        return {
            **state,
            'result': {
                'text': f'ë¯¸ëž˜ ì§ë¬´ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
            },
            'error': f'ë¯¸ëž˜ ì§ë¬´ ì¶”ì²œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}'
        }

def chat_summary(state: DevelopState) -> DevelopState:
    llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE)
    chat_summary_prompttamplate = PromptTemplate(
    input_variables=["chat_summary","user_question","answer"],
    template=chat_summary_prompt
    )
    chat_summary_llm_chain = chat_summary_prompttamplate | llm
    result = chat_summary_llm_chain.invoke({
        "chat_summary": state.get('chat_summary'),
        "user_question": state.get('input_query'),
        "answer": state.get('result')
    })
    return {**state,
        'chat_summary': result.content,
        'messages': AIMessage(result.content)}